{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUKiyh9OEUYEl3azzOT5TE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advik-2402/HumanEmotionRecognition/blob/main/Polygence_Project_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m3uzIZDN90D"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/misbah4064/facial_expressions.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd facial_expressions/\n",
        "%mkdir -p data_set/{happy,anger,surprise,sad,neutral}"
      ],
      "metadata": {
        "id": "-ApyD2WAQ8v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "\n",
        "emotions = ['happy', 'anger', 'surprise', 'sad','neutral']\n",
        "\n",
        "for emotion in emotions:\n",
        "    with open(f'{emotion}.txt','r') as f:\n",
        "        img = [line.strip() for line in f] #storing \"spaceless\" lines of text in img variable\n",
        "\n",
        "    for image in img:\n",
        "        loadedImage = cv2.imread(f\"images/{image}\") #loads images from the images folder\n",
        "        cv2.imwrite(f\"data_set/{emotion}/{image}\",loadedImage)  #saves images in the \"data_set\" folder\n",
        "    print(f\"done writing {emotion} images\")"
      ],
      "metadata": {
        "id": "tPEKAEd8Q9nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir dataset"
      ],
      "metadata": {
        "id": "D4I9sNexQ95R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# updated code to automate giving id to emotion\n",
        "emotions = {'happy': 0, 'anger': 1, 'surprise': 2, 'sad': 3, 'neutral': 4}\n",
        "\n",
        "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "for emotion in emotions:\n",
        "  with open(emotion + '.txt', 'r') as f:\n",
        "    images = [line.strip() for line in f]\n",
        "\n",
        "    face_id = emotions[emotion]\n",
        "\n",
        "    count = 0\n",
        "    \n",
        "    for image in images:\n",
        "      img = cv2.imread(\"data_set/\" + emotion + \"/\" + image) #reads the images one by one\n",
        "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #the image is converted to grayscale\n",
        "      faces = face_detector.detectMultiScale(gray, 1.3, 5) #detects faces in greyscale using pre-trained Haar Cascade\n",
        "      \n",
        "      #output is a list of rectangles that represent the bounding boxes of the detected faces in the image.\n",
        "      \n",
        "      for (x,y,w,h) in faces: #(x,y) is top left corner of the rect, w is width, h is height\n",
        "\n",
        "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2) #drawing a rect around the detected face. thickness 2, color blue.\n",
        "        count += 1 #count of faces found\n",
        "\n",
        "        # Saves the captured face data into dataset folder with face id and count.\n",
        "        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
        "\n",
        "print(\"\\n Done creating face data\", emotion)"
      ],
      "metadata": {
        "id": "Ce5WRobnQ-Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir trainer"
      ],
      "metadata": {
        "id": "tXxx5YPhQ-rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Path for face image database\n",
        "path = 'dataset'\n",
        "\n",
        "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# function to get the images and label data from the specified path\n",
        "def getImagesAndLabels(emotion_path, emotion_id):\n",
        "\n",
        "    imagePaths = [os.path.join(emotion_path,f) for f in os.listdir(emotion_path)]     \n",
        "    faceSamples=[]\n",
        "    ids = []\n",
        "\n",
        "    #The function loops over all images in the folder, detects faces using the cascade classifier, \n",
        "    #extracts the face region, and appends it to the face samples list along with the emotion ID.\n",
        "    for imagePath in imagePaths:\n",
        "        loadedImage = cv2.imread(imagePath)\n",
        "        gray = cv2.cvtColor(loadedImage, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        for (x,y,w,h) in faces:\n",
        "            faceSamples.append(gray[y:y+h,x:x+w])\n",
        "            ids.append(emotion_id)\n",
        "\n",
        "    return faceSamples,ids\n",
        "\n",
        "# Train the model on all emotions\n",
        "emotions = {\n",
        "    'happy': 0,\n",
        "    'anger': 1,\n",
        "    'surprise': 2,\n",
        "    'sad': 3,\n",
        "    'neutral': 4,\n",
        "}\n",
        "\n",
        "print (\"\\n [INFO] Training faces....\")\n",
        "\n",
        "# The all_faces and all_ids lists are used to accumulate the face samples and IDs for all emotions, \n",
        "#and these lists are passed to the train method of the recognizer to train the model on all emotions.\n",
        "\n",
        "all_faces = []\n",
        "all_ids = []\n",
        "\n",
        "for emotion, emotion_id in emotions.items():\n",
        "    emotion_path = f\"data_set/{emotion}/\"\n",
        "    faces, ids = getImagesAndLabels(emotion_path, emotion_id)\n",
        "    all_faces.extend(faces)\n",
        "    all_ids.extend(ids)\n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.train(all_faces, np.array(all_ids))\n",
        "\n",
        "# The model is saved in a yml file\n",
        "recognizer.write('trainer/trainer.yml') \n",
        "\n",
        "# The number of emotions trained is printed and program is exited\n",
        "print(\"\\n [INFO] {0} Emotions trained. Exiting Program\".format(len(np.unique(all_ids))))"
      ],
      "metadata": {
        "id": "tr5HtNf6Q--R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os \n",
        "\n",
        "# Load the trained model\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.read('trainer/trainer.yml')\n",
        "\n",
        "# Load the cascade classifier\n",
        "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "# Emotions related to ids: example ==> Happy: id=0,  etc\n",
        "names = ['Happy', 'Anger', 'Sad', 'Surprise', 'Neutral'] \n",
        "\n",
        "# Load the image\n",
        "img = cv2.imread(\"elon.jpg\")\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces in the image\n",
        "faces = faceCascade.detectMultiScale( \n",
        "    gray,\n",
        "    scaleFactor = 1.2,\n",
        "    minNeighbors = 5,\n",
        "    )\n",
        "\n",
        "# Loop through each detected face\n",
        "for (x,y,w,h) in faces:\n",
        "\n",
        "    # Draw a rectangle around the face\n",
        "    cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "\n",
        "    # Predict the emotion for the face\n",
        "    id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
        "\n",
        "    # Check if confidence is less than 100\n",
        "    if (confidence < 100):\n",
        "        id = names[id]\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "    else:\n",
        "        id = \"unknown\"\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "    \n",
        "    # Put the predicted emotion and confidence score on the image\n",
        "    cv2.putText(img, str(id), (x+5,y-5), font, 1, (255,255,255), 2)\n",
        "    cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1)  \n",
        "\n",
        "# Save the image with the predicted emotions and confidence scores\n",
        "cv2.imwrite(\"elon.jpg\",img) \n",
        "\n",
        "print(\"\\n [INFO] Done detecting and Image is saved\")"
      ],
      "metadata": {
        "id": "d11FrCIhQ_ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "image = cv2.imread(\"elon.jpg\")\n",
        "height, width = image.shape[:2]\n",
        "resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 10))\n",
        "ax.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "ax.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AkmRlAEuQ_qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "file = files.upload()"
      ],
      "metadata": {
        "id": "xkKxX29kQ_zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "music_df = pd.read_csv(\"data_moods.csv\") #reading dataset through pandas\n",
        "\n",
        "music_df = music_df[['name','artist','mood','popularity']]\n",
        "#music_df.head() #displaying the above columns of the dataset"
      ],
      "metadata": {
        "id": "_sq306ta8hce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Songs Recommendations Based on Predicted Class\n",
        "def Recommend_Songs(id):\n",
        "    \n",
        "    if( id =='Happy' ):\n",
        "\n",
        "        Play = music_df[music_df['mood'] =='Happy']\n",
        "        Play = Play.sort_values(by=\"popularity\", ascending=False)\n",
        "        Play = Play[:5].reset_index(drop=True)\n",
        "        display(Play)\n",
        "\n",
        "    elif( id =='Anger' or id =='Neutral' ):\n",
        "\n",
        "        Play = music_df[music_df['mood'] =='Calm' ]\n",
        "        Play = Play.sort_values(by=\"popularity\", ascending=False)\n",
        "        Play = Play[:5].reset_index(drop=True)\n",
        "        display(Play)\n",
        "\n",
        "    elif( id =='Sad' ):\n",
        "\n",
        "        Play = music_df[music_df['mood'] =='Sad' ]\n",
        "        Play = Play.sort_values(by=\"popularity\", ascending=False)\n",
        "        Play = Play[:5].reset_index(drop=True)\n",
        "        display(Play)\n",
        "        \n",
        "    elif( id =='Surprise'):\n",
        "\n",
        "        Play = music_df[music_df['mood'] =='Energetic' ]\n",
        "        Play = Play.sort_values(by=\"popularity\", ascending=False)\n",
        "        Play = Play[:5].reset_index(drop=True)\n",
        "        display(Play)\n",
        "\n",
        "Recommend_Songs(id)\n"
      ],
      "metadata": {
        "id": "lrvlJngd8iAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}